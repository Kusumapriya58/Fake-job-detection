{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kusumapriya58/Fake-job-detection/blob/main/Fake_job_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             precision_score, recall_score, f1_score)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# NLP utilities\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure required NLTK data is available. These calls download inside Python (not shell).\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Configuration\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.20\n",
        "MAX_FEATURES = 5000\n",
        "MODEL_OUTPUT = 'fake_job_model.pkl'\n",
        "VECTORIZER_OUTPUT = 'tfidf_vectorizer.pkl'\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def basic_text_clean(text: str) -> str:\n",
        "    \"\"\"Perform initial cleaning: lowercasing, remove HTML, URLs, punctuation, digits.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "    # Collapse whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize_and_lemmatize(text: str, lemmatizer: WordNetLemmatizer, stop_words: set) -> str:\n",
        "    \"\"\"Tokenize, remove stopwords, and lemmatize. Return cleaned string.\"\"\"\n",
        "    if not text:\n",
        "        return ''\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    cleaned_tokens = []\n",
        "    for tok in tokens:\n",
        "        tok = tok.strip()\n",
        "        # skip purely punctuation or empty\n",
        "        if not tok:\n",
        "            continue\n",
        "        if tok in stop_words:\n",
        "            continue\n",
        "        lemma = lemmatizer.lemmatize(tok)\n",
        "        cleaned_tokens.append(lemma)\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "# Part 1 — Data Understanding\n",
        "\n",
        "# 1. Load dataset\n",
        "DATA_PATH = 'fake_job_postings.csv'\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {DATA_PATH}. Place the CSV in the same folder as this script.\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Print dataset shape and columns\n",
        "print('\\n=== Dataset overview ===')\n",
        "print('Shape:', df.shape)\n",
        "print('Columns:', list(df.columns))\n",
        "\n",
        "# Missing values per column\n",
        "missing_per_col = df.isna().sum()\n",
        "print('\\nMissing values per column:')\n",
        "print(missing_per_col)\n",
        "\n",
        "# Convert label to a consistent column `fraud` if not present\n",
        "# The Kaggle dataset typically has a column named `fraudulent` with values '0'/'1' or 0/1.\n",
        "label_candidates = ['fraudulent', 'fraud', 'is_fake', 'fraudulent?']\n",
        "label_col = None\n",
        "for c in label_candidates:\n",
        "    if c in df.columns:\n",
        "        label_col = c\n",
        "        break\n",
        "\n",
        "# If no standard candidate found, inspect any column that looks like label\n",
        "if label_col is None:\n",
        "    # Heuristic: look for column with only two unique values and small unique count\n",
        "    for c in df.columns:\n",
        "        if df[c].nunique() <= 3 and df[c].dtype in [int, float, object]:\n",
        "            label_col = c\n",
        "            break\n",
        "\n",
        "if label_col is None:\n",
        "    raise ValueError('Cannot find a label column in dataset. Please ensure the dataset matches Kaggle fake_job_postings format.')\n",
        "\n",
        "# Normalize label values to 0 (real) / 1 (fake)\n",
        "print('\\nLabel column detected:', label_col)\n",
        "\n",
        "def normalize_label(v):\n",
        "    if pd.isna(v):\n",
        "        return np.nan\n",
        "    if isinstance(v, str):\n",
        "        v = v.strip()\n",
        "        if v in ('0', '0.0', 'false', 'False', 'no', 'No'):\n",
        "            return 0\n",
        "        if v in ('1', '1.0', 'true', 'True', 'yes', 'Yes'):\n",
        "            return 1\n",
        "    try:\n",
        "        nv = float(v)\n",
        "        return int(nv)\n",
        "    except Exception:\n",
        "        # fallback: treat anything non-empty as 1\n",
        "        return 1 if v else 0\n",
        "\n",
        "df['fraud'] = df[label_col].apply(normalize_label)\n",
        "\n",
        "# Distribution of fraudulent vs real\n",
        "distribution = df['fraud'].value_counts(dropna=False)\n",
        "print('\\nDistribution (fraud label counts):')\n",
        "print(distribution)\n",
        "\n",
        "# Simple insight extraction (3 short insights printed)\n",
        "print('\\n=== Three dataset observations / insights ===')\n",
        "insights = []\n",
        "# Insight 1: missing values tendency\n",
        "cols_with_many_missing = missing_per_col[missing_per_col > len(df) * 0.1].index.tolist()\n",
        "if cols_with_many_missing:\n",
        "    insights.append(f\"Columns with >10% missing values: {cols_with_many_missing}\")\n",
        "else:\n",
        "    insights.append(\"No column has more than 10% missing values.\")\n",
        "\n",
        "# Insight 2: company_profile often missing for fake jobs — heuristic check\n",
        "if 'company_profile' in df.columns:\n",
        "    cp_missing_by_fraud = df.groupby('fraud')['company_profile'].apply(lambda s: s.isna().mean())\n",
        "    insights.append(f\"Proportion of missing company_profile by label: {cp_missing_by_fraud.to_dict()}\")\n",
        "else:\n",
        "    insights.append(\"No 'company_profile' column in dataset to check missing patterns.\")\n",
        "\n",
        "# Insight 3: salary / salary_range unrealistic check (if columns exist)\n",
        "salary_cols = [c for c in df.columns if 'salary' in c.lower()]\n",
        "if salary_cols:\n",
        "    insights.append(f\"Salary-related columns detected: {salary_cols} — inspect ranges manually as needed.\")\n",
        "else:\n",
        "    insights.append(\"No explicit salary columns detected; salary may be embedded in text descriptions.\")\n",
        "\n",
        "for i, ins in enumerate(insights, 1):\n",
        "    print(f\"{i}. {ins}\")\n",
        "\n",
        "# Part 2 — Text Cleaning & Preprocessing\n",
        "print('\\n=== Part 2: Text cleaning and preprocessing ===')\n",
        "\n",
        "# We'll use the 'description' column as requested\n",
        "if 'description' not in df.columns:\n",
        "    raise ValueError(\"The dataset does not contain a 'description' column.\")\n",
        "\n",
        "# Fill NaNs with empty string for processing\n",
        "df['description'] = df['description'].fillna('')\n",
        "\n",
        "# Average word count before cleaning\n",
        "df['raw_word_count'] = df['description'].apply(lambda t: len(str(t).split()))\n",
        "avg_before = df['raw_word_count'].mean()\n",
        "print('Average word count (raw description):', round(avg_before, 2))\n",
        "\n",
        "# Initialize NLP tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Create cleaned column step-by-step\n",
        "print('Cleaning descriptions (this may take a little while depending on dataset size)...')\n",
        "# 1. Basic cleaning\n",
        "df['description_basic_clean'] = df['description'].apply(basic_text_clean)\n",
        "# 2. Tokenize, remove stopwords, lemmatize\n",
        "# Apply in a vectorized-friendly but still explicit manner\n",
        "df['clean_description'] = df['description_basic_clean'].apply(lambda t: tokenize_and_lemmatize(t, lemmatizer, stop_words))\n",
        "\n",
        "# Average word count after cleaning\n",
        "df['clean_word_count'] = df['clean_description'].apply(lambda t: len(t.split()))\n",
        "avg_after = df['clean_word_count'].mean()\n",
        "print('Average word count (cleaned):', round(avg_after, 2))\n",
        "\n",
        "# Show one raw vs cleaned sample (choose first non-empty description)\n",
        "sample_idx = df[df['description'].str.strip() != ''].index.tolist()\n",
        "if sample_idx:\n",
        "    i = sample_idx[0]\n",
        "    print('\\nExample - raw vs cleaned description (row index =', i, ')')\n",
        "    print('\\nRAW:')\n",
        "    print(df.at[i, 'description'][:1000])\n",
        "    print('\\nCLEANED:')\n",
        "    print(df.at[i, 'clean_description'][:1000])\n",
        "else:\n",
        "    print('No non-empty descriptions to display samples for.')\n",
        "\n",
        "# Part 3 — Feature Extraction (TF-IDF)\n",
        "print('\\n=== Part 3: TF-IDF feature extraction ===')\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
        "# Fit-transform on cleaned descriptions\n",
        "X_tfidf = vectorizer.fit_transform(df['clean_description'].fillna(''))\n",
        "\n",
        "print('TF-IDF matrix shape:', X_tfidf.shape)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print('\\n10 sample feature names:')\n",
        "print(list(feature_names[:10]))\n",
        "\n",
        "# Top 15 words by global TF-IDF importance (sum of TF-IDF across all documents)\n",
        "global_tfidf_sum = np.asarray(X_tfidf.sum(axis=0)).ravel()\n",
        "top15_idx = global_tfidf_sum.argsort()[::-1][:15]\n",
        "print('\\nTop 15 words by global TF-IDF score:')\n",
        "for rank, idx in enumerate(top15_idx, 1):\n",
        "    print(f\"{rank}. {feature_names[idx]} (score={global_tfidf_sum[idx]:.4f})\")\n",
        "\n",
        "# Part 4 — Model Building\n",
        "print('\\n=== Part 4: Model building & evaluation ===')\n",
        "\n",
        "# Prepare label vector — drop rows where fraud is NaN\n",
        "mask = df['fraud'].notna()\n",
        "X = X_tfidf[mask.values]\n",
        "y = df.loc[mask, 'fraud'].astype(int).values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
        "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
        "\n",
        "# Train logistic regression\n",
        "clf = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "print('\\nAccuracy:', round(acc, 4))\n",
        "print('Precision:', round(prec, 4))\n",
        "print('Recall:', round(rec, 4))\n",
        "print('F1-score:', round(f1, 4))\n",
        "\n",
        "# Confusion matrix and classification report\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('\\nConfusion matrix (rows=true, cols=predicted):')\n",
        "print(cm)\n",
        "\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# Brief interpretation (2-3 sentences) — printed as plain text lines\n",
        "print('\\nModel interpretation:')\n",
        "print('The model uses TF-IDF features and Logistic Regression. Accuracy and F1 indicate overall performance; inspect precision/recall for the fake class to understand false positives vs false negatives.')\n",
        "\n",
        "# Part 5 — Model Analysis\n",
        "print('\\n=== Part 5: Model analysis ===')\n",
        "\n",
        "# Predict_proba on 5 random examples from original dataset (that have labels)\n",
        "all_indices = df[mask].index.tolist()\n",
        "random.seed(RANDOM_STATE)\n",
        "sample_five = random.sample(all_indices, min(5, len(all_indices)))\n",
        "print('\\nFive random examples with predicted probability of being fake:')\n",
        "probs = clf.predict_proba(vectorizer.transform(df.loc[sample_five, 'clean_description']))\n",
        "for idx, p in zip(sample_five, probs):\n",
        "    prob_fake = p[1]\n",
        "    print(f\"Index {idx} — prob_fake: {prob_fake:.4f} — true label: {int(df.at[idx, 'fraud'])}\")\n",
        "\n",
        "# Manually inspect one predicted fake and one predicted real from the sample set\n",
        "# If none in sample_five suit, search the test set predictions\n",
        "preds_all = clf.predict(vectorizer.transform(df.loc[mask, 'clean_description']))\n",
        "proba_all = clf.predict_proba(vectorizer.transform(df.loc[mask, 'clean_description']))\n",
        "\n",
        "# Find one predicted fake and one predicted real (prefer misclassifications or clear cases)\n",
        "predicted_fake_indices = [i for i, p in zip(df.loc[mask].index, preds_all) if p == 1]\n",
        "predicted_real_indices = [i for i, p in zip(df.loc[mask].index, preds_all) if p == 0]\n",
        "\n",
        "inspect_examples = []\n",
        "if predicted_fake_indices:\n",
        "    inspect_examples.append(('predicted_fake', predicted_fake_indices[0]))\n",
        "if predicted_real_indices:\n",
        "    inspect_examples.append(('predicted_real', predicted_real_indices[0]))\n",
        "\n",
        "print('\\nManual inspection of one predicted fake and one predicted real:')\n",
        "for labelname, idx in inspect_examples:\n",
        "    true_label = int(df.at[idx, 'fraud'])\n",
        "    prob_fake = float(clf.predict_proba(vectorizer.transform([df.at[idx, 'clean_description']]))[0][1])\n",
        "    raw_desc = df.at[idx, 'description']\n",
        "    clean_desc = df.at[idx, 'clean_description']\n",
        "    print('\\n---')\n",
        "    print(f\"Index {idx} — {labelname} — prob_fake={prob_fake:.4f} — true_label={true_label}\")\n",
        "    print('\\nRaw description (first 700 chars):')\n",
        "    print(raw_desc[:700])\n",
        "    print('\\nCleaned description (first 700 chars):')\n",
        "    print(clean_desc[:700])\n",
        "    # Provide short textual reasoning (one sentence) about consistency\n",
        "    reasoning = 'Consistent' if (prob_fake > 0.5 and true_label == 1) or (prob_fake <= 0.5 and true_label == 0) else 'Potential mismatch'\n",
        "    print('\\nQuick assessment:', reasoning)\n",
        "\n",
        "# Persist model and vectorizer\n",
        "joblib.dump(clf, MODEL_OUTPUT)\n",
        "joblib.dump(vectorizer, VECTORIZER_OUTPUT)\n",
        "print(f\"\\nSaved model to {MODEL_OUTPUT} and vectorizer to {VECTORIZER_OUTPUT}.\")\n",
        "\n",
        "# End of script\n",
        "print('\\nPipeline complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFkuq00v00nr",
        "outputId": "8efa8669-e8eb-48f9-8ae3-e6550355a56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dataset overview ===\n",
            "Shape: (17880, 18)\n",
            "Columns: ['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements', 'benefits', 'telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education', 'industry', 'function', 'fraudulent']\n",
            "\n",
            "Missing values per column:\n",
            "job_id                     0\n",
            "title                      0\n",
            "location                 346\n",
            "department             11547\n",
            "salary_range           15012\n",
            "company_profile         3308\n",
            "description                1\n",
            "requirements            2696\n",
            "benefits                7212\n",
            "telecommuting              0\n",
            "has_company_logo           0\n",
            "has_questions              0\n",
            "employment_type         3471\n",
            "required_experience     7050\n",
            "required_education      8105\n",
            "industry                4903\n",
            "function                6455\n",
            "fraudulent                 0\n",
            "dtype: int64\n",
            "\n",
            "Label column detected: fraudulent\n",
            "\n",
            "Distribution (fraud label counts):\n",
            "fraud\n",
            "0    17014\n",
            "1      866\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Three dataset observations / insights ===\n",
            "1. Columns with >10% missing values: ['department', 'salary_range', 'company_profile', 'requirements', 'benefits', 'employment_type', 'required_experience', 'required_education', 'industry', 'function']\n",
            "2. Proportion of missing company_profile by label: {0: 0.15992711884330552, 1: 0.6778290993071594}\n",
            "3. Salary-related columns detected: ['salary_range'] — inspect ranges manually as needed.\n",
            "\n",
            "=== Part 2: Text cleaning and preprocessing ===\n",
            "Average word count (raw description): 170.45\n",
            "Cleaning descriptions (this may take a little while depending on dataset size)...\n",
            "Average word count (cleaned): 111.65\n",
            "\n",
            "Example - raw vs cleaned description (row index = 0 )\n",
            "\n",
            "RAW:\n",
            "Food52, a fast-growing, James Beard Award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its New York City headquarters.Reproducing and/or repackaging existing Food52 content for a number of partner sites, such as Huffington Post, Yahoo, Buzzfeed, and more in their various content management systemsResearching blogs and websites for the Provisions by Food52 Affiliate ProgramAssisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiriesSupporting with PR &amp; Events when neededHelping with office administrative work, such as filing, mailing, and preparing for meetingsWorking with developers to document bugs and suggest improvements to the siteSupporting the marketing and executive staff\n",
            "\n",
            "CLEANED:\n",
            "food fastgrowing james beard awardwinning online food community crowdsourced curated recipe hub currently interviewing full parttime unpaid intern work small team editor executive developer new york city headquartersreproducing andor repackaging existing food content number partner site huffington post yahoo buzzfeed various content management systemsresearching blog website provision food affiliate programassisting daytoday affiliate program support screening affiliate assisting affiliate inquiriessupporting pr amp event neededhelping office administrative work filing mailing preparing meetingsworking developer document bug suggest improvement sitesupporting marketing executive staff\n",
            "\n",
            "=== Part 3: TF-IDF feature extraction ===\n",
            "TF-IDF matrix shape: (17880, 5000)\n",
            "\n",
            "10 sample feature names:\n",
            "['aa', 'aabbf', 'aan', 'aaphone', 'ab', 'abc', 'abfceafd', 'ability', 'able', 'abreast']\n",
            "\n",
            "Top 15 words by global TF-IDF score:\n",
            "1. team (score=603.2552)\n",
            "2. customer (score=586.2711)\n",
            "3. work (score=499.4921)\n",
            "4. product (score=471.7347)\n",
            "5. service (score=467.7040)\n",
            "6. sale (score=464.2483)\n",
            "7. client (score=461.1412)\n",
            "8. business (score=434.2069)\n",
            "9. company (score=427.4724)\n",
            "10. experience (score=418.1648)\n",
            "11. looking (score=388.9820)\n",
            "12. new (score=378.6326)\n",
            "13. project (score=372.3841)\n",
            "14. job (score=354.2557)\n",
            "15. development (score=353.0522)\n",
            "\n",
            "=== Part 4: Model building & evaluation ===\n",
            "Train shape: (14304, 5000) Test shape: (3576, 5000)\n",
            "\n",
            "Accuracy: 0.9642\n",
            "Precision: 0.9787\n",
            "Recall: 0.2659\n",
            "F1-score: 0.4182\n",
            "\n",
            "Confusion matrix (rows=true, cols=predicted):\n",
            "[[3402    1]\n",
            " [ 127   46]]\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      3403\n",
            "           1       0.98      0.27      0.42       173\n",
            "\n",
            "    accuracy                           0.96      3576\n",
            "   macro avg       0.97      0.63      0.70      3576\n",
            "weighted avg       0.96      0.96      0.95      3576\n",
            "\n",
            "\n",
            "Model interpretation:\n",
            "The model uses TF-IDF features and Logistic Regression. Accuracy and F1 indicate overall performance; inspect precision/recall for the fake class to understand false positives vs false negatives.\n",
            "\n",
            "=== Part 5: Model analysis ===\n",
            "\n",
            "Five random examples with predicted probability of being fake:\n",
            "Index 3648 — prob_fake: 0.0245 — true label: 0\n",
            "Index 819 — prob_fake: 0.0416 — true label: 0\n",
            "Index 9012 — prob_fake: 0.0094 — true label: 0\n",
            "Index 8024 — prob_fake: 0.0426 — true label: 0\n",
            "Index 7314 — prob_fake: 0.0348 — true label: 0\n",
            "\n",
            "Manual inspection of one predicted fake and one predicted real:\n",
            "\n",
            "---\n",
            "Index 603 — predicted_fake — prob_fake=0.5885 — true_label=1\n",
            "\n",
            "Raw description (first 700 chars):\n",
            "Corporate overviewAker Solutions is a global provider of products, systems and services to the oil and gas industry. Our engineering, design and technology bring discoveries into production and maximize recovery from each petroleum field. We employ approximately 28,000 people in about 30 countries. Go to #URL_0fa3f7c5e23a16de16a841e368006cae916884407d90b154dfef3976483a71ae# for more information on our business, people and values.We are looking for individuals who are prepared to take a position. Not only a position within Aker Solutions, but also a position on the exciting challenges the global oil and gas industry faces now and in the future.We are looking for a Lead Mechanical Engineer to \n",
            "\n",
            "Cleaned description (first 700 chars):\n",
            "corporate overviewaker solution global provider product system service oil gas industry engineering design technology bring discovery production maximize recovery petroleum field employ approximately people country go url fa f c e de e cae b dfef ae information business people valueswe looking individual prepared take position position within aker solution also position exciting challenge global oil gas industry face futurewe looking lead mechanical engineer join team houston texasthe lead mechanical engineer responsible providing expertise technical leadership organizationresponsibilities tasks• performs mechanical calculation technical analysis various custom component review mechanical de\n",
            "\n",
            "Quick assessment: Consistent\n",
            "\n",
            "---\n",
            "Index 0 — predicted_real — prob_fake=0.0413 — true_label=0\n",
            "\n",
            "Raw description (first 700 chars):\n",
            "Food52, a fast-growing, James Beard Award-winning online food community and crowd-sourced and curated recipe hub, is currently interviewing full- and part-time unpaid interns to work in a small team of editors, executives, and developers in its New York City headquarters.Reproducing and/or repackaging existing Food52 content for a number of partner sites, such as Huffington Post, Yahoo, Buzzfeed, and more in their various content management systemsResearching blogs and websites for the Provisions by Food52 Affiliate ProgramAssisting in day-to-day affiliate program support, such as screening affiliates and assisting in any affiliate inquiriesSupporting with PR &amp; Events when neededHelping \n",
            "\n",
            "Cleaned description (first 700 chars):\n",
            "food fastgrowing james beard awardwinning online food community crowdsourced curated recipe hub currently interviewing full parttime unpaid intern work small team editor executive developer new york city headquartersreproducing andor repackaging existing food content number partner site huffington post yahoo buzzfeed various content management systemsresearching blog website provision food affiliate programassisting daytoday affiliate program support screening affiliate assisting affiliate inquiriessupporting pr amp event neededhelping office administrative work filing mailing preparing meetingsworking developer document bug suggest improvement sitesupporting marketing executive staff\n",
            "\n",
            "Quick assessment: Consistent\n",
            "\n",
            "Saved model to fake_job_model.pkl and vectorizer to tfidf_vectorizer.pkl.\n",
            "\n",
            "Pipeline complete.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOibUGk6hKvkjcbuPpX6Ukl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}